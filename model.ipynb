{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing data and data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import necesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.18452861 0.18452861 0.18452861 ... 0.43916197 0.43916197 0.44238517]\n",
      "  [0.19433962 0.19433962 0.19433962 ... 0.19245283 0.19245283 0.19245283]]\n",
      "\n",
      " [[0.19178082 0.19178082 0.19178082 ... 0.43835616 0.44319098 0.44319098]\n",
      "  [0.01320755 0.01320755 0.01320755 ... 0.15283019 0.15660377 0.15660377]]\n",
      "\n",
      " [[0.46494762 0.46494762 0.46494762 ... 0.46414182 0.46414182 0.46414182]\n",
      "  [0.81698113 0.81698113 0.81698113 ... 0.33773585 0.33207547 0.33207547]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.66720387 0.66720387 0.66720387 ... 0.53585818 0.53021757 0.53021757]\n",
      "  [0.44150943 0.44150943 0.44150943 ... 0.30566038 0.3        0.3       ]]\n",
      "\n",
      " [[0.08783239 0.08783239 0.08783239 ... 0.55922643 0.55922643 0.51087832]\n",
      "  [0.11698113 0.11698113 0.11698113 ... 0.38113208 0.38113208 0.33018868]]\n",
      "\n",
      " [[0.66800967 0.66800967 0.66720387 ... 0.50443191 0.5004029  0.5004029 ]\n",
      "  [0.09811321 0.09811321 0.1        ... 0.1490566  0.1490566  0.1490566 ]]]\n",
      "[0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Prepare training data\n",
    "training_data = []\n",
    "\n",
    "# Hold training\n",
    "with open('hold_training.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    for coordSet in data['coords']:\n",
    "        # Separate x and y coords\n",
    "        training_data.append([[coordSet['mousePos'][0::2],coordSet['mousePos'][1::2]],0]) # 0 is hold\n",
    "        \n",
    "# Spill training\n",
    "with open('spill_training.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    for coordSet in data['coords']:\n",
    "        training_data.append([[coordSet['mousePos'][0::2],coordSet['mousePos'][1::2]],1]) # 1 is spill\n",
    "\n",
    "# Shuffling\n",
    "random.shuffle(training_data)\n",
    "\n",
    "# Extract\n",
    "train_coords = []\n",
    "train_labels = []\n",
    "minValX = float('inf')\n",
    "maxValX = 0\n",
    "minValY = float('inf')\n",
    "maxValY = 0\n",
    "for sample in training_data:\n",
    "    train_coords.append(sample[0])\n",
    "    train_labels.append(sample[1])\n",
    "    minValX = min(minValX, min(sample[0][0]))\n",
    "    maxValX = max(maxValX, max(sample[0][0]))\n",
    "    minValY = min(minValY, min(sample[0][1]))\n",
    "    maxValY = max(maxValY, max(sample[0][1]))\n",
    "# Normalize\n",
    "difValX = maxValX - minValX\n",
    "difValY = maxValY - minValY\n",
    "for i in range(len(train_coords)):\n",
    "    for j in range(len(train_coords[i][0])):\n",
    "        train_coords[i][0][j] = (train_coords[i][0][j] - minValX) / float(difValX)\n",
    "        train_coords[i][1][j] = (train_coords[i][1][j] - minValY) / float(difValY)\n",
    "        \n",
    "        train_coords[i][0] = np.asarray(train_coords[i][0])\n",
    "        train_coords[i][1] = np.asarray(train_coords[i][1])\n",
    "    # Convert to np array\n",
    "    train_coords[i] = np.asarray(train_coords[i])\n",
    "\n",
    "# Convert\n",
    "train_coords = np.asarray(train_coords)\n",
    "train_labels = np.asarray(train_labels)\n",
    "print(train_coords)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2, 150)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Prepare testing data\n",
    "testing_data = []\n",
    "\n",
    "# Hold testing\n",
    "with open('hold_testing.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    for coordSet in data['coords']:\n",
    "        # Separate x and y coords\n",
    "        testing_data.append([[coordSet['mousePos'][0::2],coordSet['mousePos'][1::2]],0]) # 0 is hold\n",
    "        \n",
    "# Spill testing\n",
    "with open('spill_testing.json') as json_file:  \n",
    "    data = json.load(json_file)\n",
    "    for coordSet in data['coords']:\n",
    "        testing_data.append([[coordSet['mousePos'][0::2],coordSet['mousePos'][1::2]],1]) # 1 is spill\n",
    "\n",
    "# Shuffling\n",
    "random.shuffle(testing_data)\n",
    "\n",
    "# Extract\n",
    "test_coords = []\n",
    "test_labels = []\n",
    "minValX = float('inf')\n",
    "maxValX = 0\n",
    "minValY = float('inf')\n",
    "maxValY = 0\n",
    "for sample in testing_data:\n",
    "    test_coords.append(sample[0])\n",
    "    test_labels.append(sample[1])\n",
    "    minValX = min(minValX, min(sample[0][0]))\n",
    "    maxValX = max(maxValX, max(sample[0][0]))\n",
    "    minValY = min(minValY, min(sample[0][1]))\n",
    "    maxValY = max(maxValY, max(sample[0][1]))\n",
    "# Normalize\n",
    "difValX = maxValX - minValX\n",
    "difValY = maxValY - minValY\n",
    "for i in range(len(test_coords)):\n",
    "    for j in range(len(test_coords[i][0])):\n",
    "        test_coords[i][0][j] = (test_coords[i][0][j] - minValX) / float(difValX)\n",
    "        test_coords[i][1][j] = (test_coords[i][1][j] - minValY) / float(difValY)\n",
    "        \n",
    "        test_coords[i][0] = np.asarray(test_coords[i][0])\n",
    "        test_coords[i][1] = np.asarray(test_coords[i][1])\n",
    "    # Convert to np array\n",
    "    test_coords[i] = np.asarray(test_coords[i])\n",
    "\n",
    "# Convert\n",
    "test_coords = np.asarray(test_coords)\n",
    "test_labels = np.asarray(test_labels)\n",
    "print(test_coords.shape)\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(2,150)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Compile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 15:53:53.041600 4521743808 deprecation.py:323] From /Users/tgoh/Documents/sippy-cup/env/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20/20 [==============================] - 0s 3ms/sample - loss: 0.8531 - acc: 0.5000\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 107us/sample - loss: 0.6994 - acc: 0.5500\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 81us/sample - loss: 0.6935 - acc: 0.4000\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 99us/sample - loss: 0.6998 - acc: 0.5000\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 98us/sample - loss: 0.6711 - acc: 0.5000\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 101us/sample - loss: 0.6282 - acc: 0.6000\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 70us/sample - loss: 0.5947 - acc: 0.7500\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 59us/sample - loss: 0.5811 - acc: 0.8000\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 61us/sample - loss: 0.5779 - acc: 0.8000\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 60us/sample - loss: 0.5718 - acc: 0.8000\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 66us/sample - loss: 0.5578 - acc: 0.8000\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 53us/sample - loss: 0.5398 - acc: 0.8500\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 56us/sample - loss: 0.5233 - acc: 0.8500\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 73us/sample - loss: 0.5125 - acc: 0.9000\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 70us/sample - loss: 0.5068 - acc: 0.8000\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 57us/sample - loss: 0.5008 - acc: 0.7000\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 57us/sample - loss: 0.4915 - acc: 0.7000\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 65us/sample - loss: 0.4797 - acc: 0.8000\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 71us/sample - loss: 0.4686 - acc: 0.8500\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 76us/sample - loss: 0.4601 - acc: 0.9000\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 66us/sample - loss: 0.4550 - acc: 0.9000\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 70us/sample - loss: 0.4503 - acc: 0.9000\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 54us/sample - loss: 0.4443 - acc: 0.9000\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 52us/sample - loss: 0.4371 - acc: 0.9000\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 69us/sample - loss: 0.4298 - acc: 0.9000\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 71us/sample - loss: 0.4236 - acc: 0.8500\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 53us/sample - loss: 0.4185 - acc: 0.8500\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 57us/sample - loss: 0.4139 - acc: 0.8500\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 60us/sample - loss: 0.4089 - acc: 0.8500\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 63us/sample - loss: 0.4033 - acc: 0.8500\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 68us/sample - loss: 0.3973 - acc: 0.8500\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 70us/sample - loss: 0.3916 - acc: 0.9000\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 59us/sample - loss: 0.3869 - acc: 0.9000\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 56us/sample - loss: 0.3823 - acc: 0.9000\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 55us/sample - loss: 0.3782 - acc: 0.9000\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 70us/sample - loss: 0.3735 - acc: 0.9000\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 65us/sample - loss: 0.3682 - acc: 0.9000\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 53us/sample - loss: 0.3631 - acc: 0.9000\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 78us/sample - loss: 0.3586 - acc: 0.8500\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 59us/sample - loss: 0.3545 - acc: 0.8500\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 57us/sample - loss: 0.3504 - acc: 0.8500\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 57us/sample - loss: 0.3459 - acc: 0.8500\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 76us/sample - loss: 0.3414 - acc: 0.9000\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 76us/sample - loss: 0.3370 - acc: 0.9000\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 71us/sample - loss: 0.3329 - acc: 0.9000\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 59us/sample - loss: 0.3287 - acc: 0.9000\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 61us/sample - loss: 0.3244 - acc: 0.9000\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 68us/sample - loss: 0.3202 - acc: 0.9000\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 56us/sample - loss: 0.3158 - acc: 0.9000\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 55us/sample - loss: 0.3113 - acc: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13bdb2320>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_coords, train_labels, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "20/20 [==============================] - 0s 856us/sample - loss: 0.4722 - acc: 0.8000\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_coords, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08783162]\n",
      " [0.21434844]\n",
      " [0.8796797 ]\n",
      " [0.6541165 ]\n",
      " [0.9894339 ]\n",
      " [0.39679602]\n",
      " [0.59016925]\n",
      " [0.40661946]\n",
      " [0.1633619 ]\n",
      " [0.5297847 ]\n",
      " [0.58492196]\n",
      " [0.20273101]\n",
      " [0.22926798]\n",
      " [0.37387815]\n",
      " [0.45531476]\n",
      " [0.55732757]\n",
      " [0.08413476]\n",
      " [0.4441853 ]\n",
      " [0.71137273]\n",
      " [0.15819786]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_coords)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python model\n",
    "model.save('Model/model.h5')\n",
    "# JS model\n",
    "tfjs.converters.save_keras_model(model, './ModelJS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
